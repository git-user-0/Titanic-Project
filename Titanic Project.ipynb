{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Titanic Survivors\n",
    "\n",
    "This is my attempt at predicting who survived the Titanic disaster. It is in response to the competition set on <a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\">kaggle</a>.\n",
    "\n",
    "It has enabled me to learn more about machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "The first thing my program does is import the various libraries and functions I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import itertools\n",
    "\n",
    "from numpy import set_printoptions\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Data Frames\n",
    "\n",
    "Next, I import the data and put them into X_full and y. X_full does not contain the 'Survived' column because the values in it are the ones I am trying to predict. These values are stored in y.\n",
    "\n",
    "I drop 'PassengerId' from X_full as I don't think the passenger Id numbers would have any connection to whether or not a person survived.\n",
    "\n",
    "Then, I split the data into a training set of 80% and a validation set of 20%. I do this before I preprocess the data so as to avoid any data leakage which would give me unreliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "# load the data\n",
    "data_set = pd.read_csv('/home/bodhi/Documents/Jupyter/Titanic/train.csv')\n",
    "\n",
    "# set up X_full and y\n",
    "X_full = data_set.drop(columns=['Survived', 'PassengerId']).copy()\n",
    "y = data_set.Survived.copy()\n",
    "\n",
    "# split the data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal With Preprocessing of Data\n",
    "\n",
    "Next, I set up pipelines to deal with the preprocessing of categorical and numerical features.\n",
    "\n",
    "The categorical features have Nan types filled with the most frequent value before they are count-encoded.\n",
    "\n",
    "The numerical features have Nan types filled with the median value of their column before they are standardized and normalized.\n",
    "\n",
    "I also create an initial model and put it into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numerical columns which need to be preprocessed\n",
    "numerical_cols = [cname for cname in X_train_full.columns if\n",
    "                 X_train_full[cname].dtype in ['int64', 'float64']\n",
    "                 ]\n",
    "\n",
    "# select categorical columns which need to be preprocessed\n",
    "categorical_cols = [cname for cname in X_train_full.columns\n",
    "                  if X_train_full[cname].nunique() < 10 and\n",
    "                  X_train_full[cname].dtype == 'object'\n",
    "                   ]\n",
    "\n",
    "# preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('count_enc', ce.CountEncoder())\n",
    "]\n",
    "                                  )\n",
    "\n",
    "# preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('normalize', MinMaxScaler())\n",
    "]\n",
    "                                   )\n",
    "\n",
    "# set up the initial model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# put the model into a pipeline\n",
    "pipe = Pipeline(steps=[('model', model)\n",
    "                      ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Training Data\n",
    "\n",
    "Now, I use a function to transform the training data. This function uses the pipelines created earlier to preprocess the categorical and numerical features found in the training data.\n",
    "\n",
    "The function uses a parameter to recognise if it is working on the training data or not. If it is working on the training data, it fits the preprocessing pipelines to the data before transforming it.\n",
    "\n",
    "If it is not working on the training data, it must be working on the validation or test data. This means that it only transforms the data using the pipelines which have been fitted to the training data. This is to eliminate data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, split):\n",
    "    # impute missing categorical data and count encode it\n",
    "    set_cats = pd.DataFrame(df[categorical_cols])\n",
    "    if split == 'train':\n",
    "        set_cats = categorical_transformer.fit_transform(set_cats)\n",
    "    else:\n",
    "        set_cats = categorical_transformer.transform(set_cats)\n",
    "    set_cats.columns = categorical_cols\n",
    "    set_cats.index = df.index\n",
    "    \n",
    "    # impute missing numerical data\n",
    "    imputed_df_nums = pd.DataFrame(df[numerical_cols])\n",
    "    if split == 'train':\n",
    "        imputed_df_nums = numerical_transformer.fit_transform(imputed_df_nums)\n",
    "    else:\n",
    "        imputed_df_nums = numerical_transformer.transform(imputed_df_nums)\n",
    "    imputed_df_nums = pd.DataFrame(imputed_df_nums, columns=[numerical_cols])\n",
    "    imputed_df_nums.index = df.index\n",
    "\n",
    "    # create a new data frame using the transformed training data\n",
    "    frames = [set_cats, imputed_df_nums]\n",
    "    baseline = set_cats.join(imputed_df_nums, how='inner')\n",
    "    return baseline\n",
    "\n",
    "# transform the training data with imputation and count encoding\n",
    "baseline_train = transform(X_train_full, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Interactions\n",
    "\n",
    "Next, new interactions are created by combining each categorical feature with another categorical feature.\n",
    "\n",
    "The new interactions are joined to the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions(baseline):\n",
    "    # create interactions by combining categorical features\n",
    "    interactions = pd.DataFrame(index=baseline.index)\n",
    "    for col1, col2 in itertools.combinations(categorical_cols, 2):\n",
    "        new_col_name = '_'.join([col1, col2])\n",
    "        # combine\n",
    "        new_values = baseline[col1] - baseline[col2]\n",
    "        interactions[new_col_name] = new_values\n",
    "    return interactions\n",
    "\n",
    "# create new interactions\n",
    "new_features = get_interactions(baseline_train)\n",
    "\n",
    "# combine the transformed numerical and categorical data\n",
    "baseline_train = baseline_train.join(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best Features\n",
    "\n",
    "Now, we find the best number of features using SelectKBest. Linear relationships gave better results than when I included non-linear ones, so I use f_classif.\n",
    "\n",
    "This part of the program returns the smallest number for k which has the best score.\n",
    "\n",
    "A new data frame is created (X_train_best) which only uses the best features (as found using SelectKBest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best(k):\n",
    "    # keep best features\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_new = selector.fit_transform(baseline_train, y_train)\n",
    "    \n",
    "    # get back the best features and zero out all the others\n",
    "    selected_features = pd.DataFrame(selector.inverse_transform(X_new),\n",
    "                                     index=baseline_train.index,\n",
    "                                     columns=feature_cols)\n",
    "\n",
    "    # features other than the best ones have values of 0s, so var is 0 - drop them\n",
    "    selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "    return selected_columns\n",
    "\n",
    "def get_k():\n",
    "    k = 1\n",
    "    results = []\n",
    "    while k <= 7:\n",
    "        best_columns = get_best(k)\n",
    "        test_best = baseline_train[best_columns].copy()\n",
    "        results.append(cross_val_score(pipe, test_best, y_train, scoring='accuracy').mean())\n",
    "        k+=1\n",
    "    return (results.index(max(results))) + 1\n",
    "\n",
    "# find the best number of features (k)\n",
    "feature_cols = baseline_train.columns\n",
    "k = get_k()\n",
    "\n",
    "# find the best features\n",
    "best_columns = get_best(k)\n",
    "\n",
    "# create an X_train set of data which uses only the best features\n",
    "X_train_best = baseline_train[best_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Validation Data\n",
    "\n",
    "The validation part of the original data is now preprocessed and new interactions are found for it. A new data frame is created (X_valid_best) which only uses the best features (as found using SelectKBest on the training data earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the validation data with imputation and count encoding\n",
    "baseline_valid = transform(X_valid_full, 'valid')\n",
    "\n",
    "# create new interactions\n",
    "new_features_valid = get_interactions(baseline_valid)\n",
    "\n",
    "# combine the transformed numerical and categorical data\n",
    "baseline_valid = baseline_valid.join(new_features_valid)\n",
    "\n",
    "# create an X_valid set of data which uses only the best features\n",
    "X_valid_best = baseline_valid[best_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Check Algorithms\n",
    "\n",
    "We are now ready to spot-check linear, non-linear and ensemble classifiers. The mean results are printed along with the standard deviation.\n",
    "\n",
    "To make the performances easier to understand, a box-plot is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.796405 (0.020735)\n",
      "KNN-: 0.799222 (0.023184)\n",
      "CART: 0.754309 (0.030597)\n",
      "RF: 0.823146 (0.039057)\n",
      "XGB: 0.828721 (0.025663)\n",
      "SVM: 0.613799 (0.016887)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_models(models=[]):\n",
    "    # linear models\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    \n",
    "    # non-linear models\n",
    "    models.append(('KNN-', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    \n",
    "    # ensemble models\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    \n",
    "    return models\n",
    "\n",
    "def spot_check():\n",
    "    models = get_models()\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, classifier in models:\n",
    "        pipe.set_params(model = classifier)\n",
    "        cv_results = cross_val_score(pipe, X_train_best, y_train,scoring='accuracy')\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    # plot model performance for comparison\n",
    "    plt.boxplot(results, labels=names, showmeans=True)\n",
    "    plt.show()\n",
    "    \n",
    "# spot-check algorithms and show performances\n",
    "spot_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "I continue to use XGBoost as it performs the best on the training data.\n",
    "\n",
    "Using grid search, I hyper-tune some basic parameters.\n",
    "\n",
    "The best model is created using the best values found for the basic parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Best Model on the Validation Data\n",
    "\n",
    "The best model is then used to make predictions about the validation data and an accuracy score returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Test Data and Make the Final Predictions\n",
    "\n",
    "Next, the test data is loaded and preprocessed. New interactions are created and the best model uses the best features to make predictions for the test data.\n",
    "\n",
    "Finally, the predictions are turned into an excel file and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning...\n",
      "{'model__learning_rate': 0.1, 'model__n_estimators': 500}\n",
      "\n",
      "Best Model Accuracy Score: 0.7932960893854749\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "def grid_search():\n",
    "    print('Tuning...')\n",
    "    pipe.set_params(model=XGBClassifier())\n",
    "    param_grid = dict(model__learning_rate=[0.1],\n",
    "                      model__n_estimators=[500]\n",
    "                     )\n",
    "    grid = GridSearchCV(pipe,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=3,\n",
    "                        scoring='accuracy')\n",
    "    grid.fit(X_train_best, y_train)\n",
    "    print(grid.best_params_)\n",
    "    return grid\n",
    "\n",
    "# search for the best parameters using gridsearch and\n",
    "# create the best model using the best parameters\n",
    "grid = grid_search()\n",
    "\n",
    "best_learning = grid.best_params_['model__learning_rate']\n",
    "best_estimators = grid.best_params_['model__n_estimators']\n",
    "\n",
    "pipe.set_params(model=XGBClassifier(learning_rate=best_learning,\n",
    "                                   n_estimators=best_estimators\n",
    "                                   )\n",
    "               )\n",
    "\n",
    "# use the best model to make a prediciton on validation data\n",
    "best_model = pipe.fit(X_train_best, y_train)\n",
    "y_pred = best_model.predict(X_valid_best)\n",
    "print('\\nBest Model Accuracy Score:', metrics.accuracy_score(y_valid, y_pred))\n",
    "\n",
    "# load the new data\n",
    "mystery_data_set = pd.read_csv('/home/bodhi/Documents/Jupyter/Titanic/test_4.csv')\n",
    "\n",
    "mx = mystery_data_set.copy()\n",
    "\n",
    "# transform the training data with imputation and count encoding\n",
    "baseline_test = transform(mx, 'test')\n",
    "\n",
    "# create new interactions\n",
    "new_features = get_interactions(baseline_test)\n",
    "\n",
    "# combine the transformed numerical and categorical data\n",
    "baseline_test = baseline_test.join(new_features)\n",
    "\n",
    "# create an X_test set of data which uses only the best features\n",
    "X_test_best = baseline_test[best_columns]\n",
    "\n",
    "# predict the final values\n",
    "preds = best_model.predict(X_test_best)\n",
    "\n",
    "# load the submission information\n",
    "my_solution = pd.DataFrame({'PassengerId':mystery_data_set['PassengerId'],\n",
    "                                'Survived':preds})\n",
    "\n",
    "my_solution.to_excel('/home/bodhi/Documents/Jupyter/Titanic/tt_2.xls')\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results?\n",
    "\n",
    "I submitted the predictions to the <a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\">kaggle</a> competition and scored:\n",
    "\n",
    "0.76076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Accuracy\n",
    "\n",
    "I think that the best way to improve accuracy would be to look again at the data. I have noticed that the feature 'Parch' combines data about whether or not a passenger had any parents or children. I think these data would be better if they were seperated. In order to do this, I will need to do some online research. It might increase the accuracy of my program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
